# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uVmeNX6HPDYsa91CROK8tP5uvxyIX3gW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from scipy.stats import pearsonr

df = pd.read_excel('/content/Extract.xlsx')

t = 10000
x = np.linspace(1,t,t)
x1=np.array(df['x1'][:t])
x2=np.array(df['x2'][:t])
x3=np.array(df['x3'][:t])
x4=np.array(df['x4'][:t])
x1.shape,x2.shape,x3.shape,x4.shape

#diff = x2[1:]-x2[:t-1] #x2=.35,x3=0.43,x4=0.64
a = np.array([x1,x2,x3,x4]).T
a

from sklearn.linear_model import LogisticRegression

"""**Model**"""

s1 = (a[3:t]-a[2:t-1])[:,3]
s2 = (a[3:t]-a[1:t-2])[:,3]
s3 = (a[3:t]-a[:t-3])[:,3]
r4 = (a[2:t-1][:,0]-a[2:t-1][:,3])
h5 = a[2:t-1][:,1]-a[2:t-1][:,2]
s2 = s2/2
s3 = s3/3

y = np.array(df['y'])[3:]

w = int(t*0.8)
f = np.ones((s1.shape[0],5))
f[:,0] = s1
f[:,1] = s2
f[:,2] = s3
f[:,3] = r4
f[:,4] = h5
train = f[:w]

train

m = LogisticRegression()
m.fit(train,y[:w])

m.score(f[w:],y[w:])



#pickle h5
import pickle
file_name='my_file.pkl'
f = open(file_name,'wb')
pickle.dump(m,f)
f.close()

with open('/content/my_file.pkl', 'rb') as fid:
  data3 = pickle.load(fid)
data3

"""testing"""

df = pd.read_excel('/content/test_data_Analytics GC.xlsx')
df.head()

x1=np.array(df['x1'][:t])
x2=np.array(df['x2'][:t])
x3=np.array(df['x3'][:t])
x4=np.array(df['x4'][:t])

a = np.array([x1,x2,x3,x4]).T
a

t=1000
s1 = (a[3:t]-a[2:t-1])[:,3]
s2 = (a[3:t]-a[1:t-2])[:,3]
s3 = (a[3:t]-a[:t-3])[:,3]
r4 = (a[2:t-1][:,0]-a[2:t-1][:,3])
h5 = a[2:t-1][:,1]-a[2:t-1][:,2]
s2 = s2/2
s3 = s3/3

f = np.ones((s1.shape[0],5))
f[:,0] = s1
f[:,1] = s2
f[:,2] = s3
f[:,3] = r4
f[:,4] = h5

pd.DataFrame(m.predict(f)).to_csv('pred.csv')

d = pd.read_csv('/content/pred.csv')
d



#3:

# n-1,n-2,n-3 days ka slope (closing price) (slope = close(n)-close(n-k))
# n-1 day ka close-open (return)
# n-1 day high - low
# prediction day 3
# close-open > 0 = 1 else 0

#m.predict()

